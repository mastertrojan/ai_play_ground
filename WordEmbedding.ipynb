{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download reviews.txt and labels.txt from here: https://github.com/udacity/deep-learning/tree/master/sentiment-network\n",
    "\n",
    "def pretty_print_review_and_label(i):\n",
    "   print(labels[i] + \"\\t:\\t\" + reviews[i][:80] + \"...\")\n",
    "\n",
    "g = open('reviews.txt','r') # What we know!\n",
    "reviews = list(map(lambda x:x[:-1],g.readlines()))\n",
    "g.close()\n",
    "\n",
    "g = open('labels.txt','r') # What we WANT to know!\n",
    "labels = list(map(lambda x:x[:-1].upper(),g.readlines()))\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain: \n",
      " [array([0, 1, 0, 0]), array([1, 0, 0, 0]), array([0, 0, 0, 1])] \n",
      "\n",
      "Numpy Array: \n",
      " [[0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 1]] \n",
      "\n",
      "Weights: \n",
      " [[-0.01818611 -0.07525092 -0.08364671 -0.04150304]\n",
      " [-0.03554806  0.00544011  0.05412751 -0.07615839]\n",
      " [ 0.07642483  0.06649604 -0.06632006  0.05392021]\n",
      " [ 0.05576549  0.08386976  0.05442845 -0.0855062 ]] \n",
      "\n",
      "Embedded layer: \n",
      " [-0.12483028 -0.0589306   0.07873583 -0.26997822]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "onehots = {}\n",
    "onehots['cat'] = np.array([1,0,0,0])\n",
    "onehots['the'] = np.array([0,1,0,0])\n",
    "onehots['dog'] = np.array([0,0,1,0])\n",
    "onehots['sat'] = np.array([0,0,0,1])\n",
    "\n",
    "sentence = ['the','cat','sat']\n",
    "x = onehots[sentence[0]] + \\\n",
    "    onehots[sentence[1]] + \\\n",
    "    onehots[sentence[2]]\n",
    "\n",
    "input = list()\n",
    "for word in sentence:\n",
    "    input.append(onehots[word])\n",
    "print('Plain:','\\n', input, '\\n')\n",
    "input_np = np.array([np.array(input_i) for input_i in input])\n",
    "print('Numpy Array:','\\n', input_np, '\\n')\n",
    "#input_flat = input_np.flatten()\n",
    "#print('Flatten: ', input_flat)\n",
    "test_array = np.array([1,1,0,1])\n",
    "weights_test = 0.2* np.random.random((len(onehots),4)) - 0.1 \n",
    "print(\"Weights:\",'\\n', weights_test, '\\n')\n",
    "\n",
    "print('Embedded layer:','\\n', np.sum(weights_test[test_array], axis=0 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word \"terrible\" has the index: 58788\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "f = open('reviews.txt')\n",
    "raw_reviews = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open('labels.txt')\n",
    "raw_labels = f.readlines()\n",
    "f.close()\n",
    "# create tokens by creating arrays of words by seperating them by spaces\n",
    "tokens = list(map(lambda x:set(x.split(\" \")),raw_reviews))\n",
    "\n",
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        if(len(word)>0):\n",
    "            vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "#Enumerate the words so neural net can perform computations, in NN terms One Hot Encode the data\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "print('The word \"terrible\" has the index:',word2index['terrible'])\n",
    "input_dataset = list()\n",
    "for sent in tokens:\n",
    "    sent_indices = list()\n",
    "    for word in sent:\n",
    "        try:\n",
    "            sent_indices.append(word2index[word])\n",
    "        except:\n",
    "            \"\"\n",
    "    input_dataset.append(list(set(sent_indices)))\n",
    "target_dataset = list()\n",
    "\n",
    "for label in raw_labels:\n",
    "    if label == 'positive\\n':\n",
    "        target_dataset.append(1)\n",
    "    else:\n",
    "        target_dataset.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1c0e7976ccc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mweights_0_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mweights_1_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "np.random.seed(1) \n",
    "\n",
    "def sigmoid(x): \n",
    "    return 1/(1 + np.exp(-x)) \n",
    "\n",
    "alpha, iterations = (0.01, 2) \n",
    "hidden_size = 100 \n",
    "\n",
    "weights_0_1 = 0.2*np.random.random((len(vocab),hidden_size)) - 0.1 \n",
    "weights_1_2 = 0.2*np.random.random((hidden_size,1)) - 0.1 \n",
    "\n",
    "correct,total = (0,0)\n",
    "for iter in range(iterations): \n",
    "    # train on first 24,000 \n",
    "    for i in range(len(input_dataset)-1000): \n",
    "        x,y = (input_dataset[i],target_dataset[i]) \n",
    "        np_sum = np.sum(weights_0_1[x],axis=0)\n",
    "        #print('sum:{} , size:{}'.format(np_sum, np_sum.shape))\n",
    "        layer_1 = sigmoid(np_sum) #embed + sigmoid \n",
    "        #print('sigmoid: {}, shape: {}'.format(layer_1, layer_1.shape))\n",
    "        layer_2 = sigmoid(np.dot(layer_1,weights_1_2)) # linear + softmax \n",
    "        layer_2_delta = layer_2 - y # compare pred with truth \n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) #backprop \n",
    "        weights_0_1[x] -= layer_1_delta * alpha \n",
    "        weights_1_2 -= np.outer(layer_1,layer_2_delta) * alpha \n",
    "\n",
    "        if(np.abs(layer_2_delta) < 0.5): \n",
    "            correct += 1 \n",
    "            total += 1 \n",
    "            if(i % 10 == 9): \n",
    "                progress = str(i/float(len(input_dataset))) \n",
    "                sys.stdout.write('\\rIter:' + str(iter)\\\n",
    "                                 +' Progress:' + progress[2:4]\\\n",
    "                                 +'.' + progress[4:6]\\\n",
    "                                 +'% Training Accuracy:'\\\n",
    "                                 + str(correct/float(total)) + '%') \n",
    "    print() \n",
    "correct,total = (0,0) \n",
    "for i in range(len(input_dataset)-1000,len(input_dataset)): \n",
    "    x = input_dataset[i] \n",
    "    y = target_dataset[i] \n",
    "    layer_1 = sigmoid(np.sum(weights_0_1[x],axis=0)) \n",
    "    layer_2 = sigmoid(np.dot(layer_1,weights_1_2)) \n",
    "    if(np.abs(layer_2 - y) < 0.5): \n",
    "        correct += 1 \n",
    "    total += 1 \n",
    "print(\"Test Accuracy:\" + str(correct / float(total))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aind]",
   "language": "python",
   "name": "conda-env-aind-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
